@article{ansariMixtureModelsPhotometric2021,
  title = {Mixture Models for Photometric Redshifts},
  author = {Ansari, Zoe and Agnello, Adriano and Gall, Christa},
  year = {2021},
  month = jun,
  journal = {Astronomy \& Astrophysics},
  volume = {650},
  pages = {A90},
  issn = {0004-6361, 1432-0746},
  doi = {10.1051/0004-6361/202039675},
  urldate = {2022-02-21},
  abstract = {Methods. We performed a probabilistic photo-z determination using mixture density networks (MDN). The training data set is composed of optical (griz photometric bands) point-spread-function and model magnitudes and extinction measurements from the SDSSDR15 and WISE mid-infrared (3.4 \textmu m and 4.6 \textmu m) model magnitudes. We used infinite Gaussian mixture models to classify the objects in our data set as stars, galaxies, or quasars, and to determine the number of MDN components to achieve optimal performance. Results. The fraction of objects that are correctly split into the main classes of stars, galaxies, and quasars is 94\%. Furthermore, our method improves the bias of photometric redshift estimation (i.e., the mean {$\increment$}z = (zp - zs)/(1 + zs)) by one order of magnitude compared to the SDSS photo-z, and it decreases the fraction of 3{$\sigma$} outliers (i.e., 3 \texttimes{} rms({$\increment$}z) {$<$} {$\increment$}z). The relative, root-mean-square systematic uncertainty in our resulting photo-zs is down to 1.7\% for benchmark samples of low-redshift galaxies (zs {$<$} 0.5). Conclusions. We have demonstrated the feasibility of machine-learning-based methods that produce full probability distributions for photo-z estimates with a performance that is competitive with state-of-the art techniques. Our method can be applied to wide-field surveys where extinction can vary significantly across the sky and with sparse spectroscopic calibration samples. The code is publicly available.},
  langid = {english},
  file = {/home/jacob/snap/zotero-snap/common/Zotero/storage/LMYTG4NQ/Ansari et al. - 2021 - Mixture models for photometric redshifts.pdf}
}

@article{bessellStandardPhotometricSystems2005,
  title = {Standard {{Photometric Systems}}},
  author = {Bessell, Michael S.},
  year = {2005},
  month = sep,
  journal = {Annual Review of Astronomy and Astrophysics},
  volume = {43},
  number = {1},
  pages = {293--336},
  issn = {0066-4146, 1545-4282},
  doi = {10.1146/annurev.astro.41.082801.100251},
  urldate = {2022-10-20},
  abstract = {{$\blacksquare$} Abstract\hspace{0.6em} Standard star photometry dominated the latter half of the twentieth century reaching its zenith in the 1980s. It was introduced to take advantage of the high sensitivity and large dynamic range of photomultiplier tubes compared to photographic plates. As the quantum efficiency of photodetectors improved and the wavelength range extended further to the red, standard systems were modified and refined, and deviations from the original systems proliferated. The revolutionary shift to area detectors for all optical and IR observations forced further changes to standard systems, and the precision and accuracy of much broad- and intermediate-band photometry suffered until more suitable observational techniques and standard reduction procedures were adopted. But the biggest revolution occurred with the production of all-sky photometric surveys. Hipparcos/Tycho was space based, but most, like 2MASS, were ground-based, dedicated survey telescopes. It is very likely that in the future, rather than making a measurement of an object in some standard photometric system, one will simply look up the magnitudes and colors of most objects in catalogs accessed from the Virtual Observatory. In this review the history of standard star photometry will be outlined, and the calibration and realization of standard systems will be examined. Finally, model atmosphere fluxes are now very realistic, and synthetic photometry offers the best prospects for calibrating all photometric systems. Synthetic photometry from observed spectrophotometry should also be used as a matter of course to provide colors within standard systems and to gain insights into the spectra and colors of unusual stars, star clusters and distant galaxies.},
  langid = {english},
  file = {/home/jacob/snap/zotero-snap/common/Zotero/storage/2IDREGSJ/Bessell - 2005 - Standard Photometric Systems.pdf}
}

@book{bishopPatternRecognitionMachine2006,
  title = {Pattern Recognition and Machine Learning},
  author = {Bishop, Christopher M.},
  year = {2006},
  series = {Information Science and Statistics},
  publisher = {{Springer}},
  address = {{New York}},
  isbn = {978-0-387-31073-2},
  langid = {english},
  lccn = {Q327 .B52 2006},
  keywords = {Machine learning,Pattern perception},
  file = {/home/jacob/snap/zotero-snap/common/Zotero/storage/Y4DNQ5GY/Bishop - 2006 - Pattern recognition and machine learning.pdf}
}

@misc{childVeryDeepVAEs2021,
  title = {Very {{Deep VAEs Generalize Autoregressive Models}} and {{Can Outperform Them}} on {{Images}}},
  author = {Child, Rewon},
  year = {2021},
  month = mar,
  number = {arXiv:2011.10650},
  eprint = {2011.10650},
  primaryclass = {cs},
  publisher = {{arXiv}},
  urldate = {2023-10-11},
  abstract = {We present a hierarchical VAE that, for the first time, generates samples quickly and outperforms the PixelCNN in log-likelihood on all natural image benchmarks. We begin by observing that, in theory, VAEs can actually represent autoregressive models, as well as faster, better models if they exist, when made sufficiently deep. Despite this, autoregressive models have historically outperformed VAEs in loglikelihood. We test if insufficient depth explains why by scaling a VAE to greater stochastic depth than previously explored and evaluating it CIFAR-10, ImageNet, and FFHQ. In comparison to the PixelCNN, these very deep VAEs achieve higher likelihoods, use fewer parameters, generate samples thousands of times faster, and are more easily applied to high-resolution images. Qualitative studies suggest this is because the VAE learns efficient hierarchical visual representations. We release our source code and models at https://github.com/openai/vdvae.},
  archiveprefix = {arxiv},
  langid = {english},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning},
  file = {/home/jacob/snap/zotero-snap/common/Zotero/storage/M86TG9MD/Child - 2021 - Very Deep VAEs Generalize Autoregressive Models an.pdf}
}

@inproceedings{cohenNoTitleFound2006,
  title = {[{{No}} Title Found]},
  booktitle = {Proceedings of the 23rd International Conference on {{Machine}} Learning  - {{ICML}} '06},
  year = {2006},
  publisher = {{ACM Press}},
  address = {{Pittsburgh, Pennsylvania}},
  abstract = {Variational autoencoders are a powerful framework for unsupervised learning. However, previous work has been restricted to shallow models with one or two layers of fully factorized stochastic latent variables, limiting the flexibility of the latent representation. We propose three advances in training algorithms of variational autoencoders, for the first time allowing to train deep models of up to five stochastic layers, (1) using a structure similar to the Ladder network as the inference model, (2) warm-up period to support stochastic units staying active in early training, and (3) use of batch normalization. Using these improvements we show state-of-the-art log-likelihood results for generative modeling on several benchmark datasets.},
  collaborator = {Cohen, William and Moore, Andrew},
  isbn = {978-1-59593-383-6},
  langid = {english},
  file = {/home/jacob/snap/zotero-snap/common/Zotero/storage/BIGHI3X4/Cohen and Moore - 2006 - [No title found].pdf}
}

@article{dahlenCRITICALASSESSMENTPHOTOMETRIC2013,
  title = {A {{CRITICAL ASSESSMENT OF PHOTOMETRIC REDSHIFT METHODS}}: {{A CANDELS INVESTIGATION}}},
  shorttitle = {A {{CRITICAL ASSESSMENT OF PHOTOMETRIC REDSHIFT METHODS}}},
  author = {Dahlen, Tomas and Mobasher, Bahram and Faber, Sandra M. and Ferguson, Henry C. and Barro, Guillermo and Finkelstein, Steven L. and Finlator, Kristian and Fontana, Adriano and Gruetzbauch, Ruth and Johnson, Seth and Pforr, Janine and Salvato, Mara and Wiklind, Tommy and Wuyts, Stijn and Acquaviva, Viviana and Dickinson, Mark E. and Guo, Yicheng and Huang, Jiasheng and Huang, Kuang-Han and Newman, Jeffrey A. and Bell, Eric F. and Conselice, Christopher J. and Galametz, Audrey and Gawiser, Eric and Giavalisco, Mauro and Grogin, Norman A. and Hathi, Nimish and Kocevski, Dale and Koekemoer, Anton M. and Koo, David C. and Lee, Kyoung-Soo and McGrath, Elizabeth J. and Papovich, Casey and Peth, Michael and Ryan, Russell and Somerville, Rachel and Weiner, Benjamin and Wilson, Grant},
  year = {2013},
  month = sep,
  journal = {The Astrophysical Journal},
  volume = {775},
  number = {2},
  pages = {93},
  issn = {0004-637X, 1538-4357},
  doi = {10.1088/0004-637X/775/2/93},
  urldate = {2022-10-23},
  abstract = {We present results from the Cosmic Assembly Near-infrared Deep Extragalactic Legacy Survey (CANDELS) photometric redshift methods investigation. In this investigation, the results from 11 participants, each using a different combination of photometric redshift code, template spectral energy distributions (SEDs), and priors, are used to examine the properties of photometric redshifts applied to deep fields with broadband multi-wavelength coverage. The photometry used includes U-band through mid-infrared filters and was derived using the TFIT method. Comparing the results, we find that there is no particular code or set of template SEDs that results in significantly better photometric redshifts compared to others. However, we find that codes producing the lowest scatter and outlier fraction utilize a training sample to optimize photometric redshifts by adding zero-point offsets, template adjusting, or adding extra smoothing errors. These results therefore stress the importance of the training procedure. We find a strong dependence of the photometric redshift accuracy on the signal-to-noise ratio of the photometry. On the other hand, we find a weak dependence of the photometric redshift scatter with redshift and galaxy color. We find that most photometric redshift codes quote redshift errors (e.g., 68\% confidence intervals) that are too small compared to that expected from the spectroscopic control sample. We find that all codes show a statistically significant bias in the photometric redshifts. However, the bias is in all cases smaller than the scatter; the latter therefore dominates the errors. Finally, we find that combining results from multiple codes significantly decreases the photometric redshift scatter and outlier fraction. We discuss different ways of combining data to produce accurate photometric redshifts and error estimates.},
  langid = {english},
  file = {/home/jacob/snap/zotero-snap/common/Zotero/storage/IVWGF62H/Dahlen et al. - 2013 - A CRITICAL ASSESSMENT OF PHOTOMETRIC REDSHIFT METH.pdf}
}

@misc{dainottiReducedUncertainties432023,
  title = {Reduced Uncertainties up to 43\textbackslash\% on the {{Hubble}} Constant and the Matter Density with the {{SNe Ia}} with a New Statistical Analysis},
  author = {Dainotti, Maria Giovanna and Bargiacchi, Giada and Nagataki, Shigehiro and Bogdan, Malgorzata and Capozziello, Salvatore},
  year = {2023},
  month = mar,
  number = {arXiv:2303.06974},
  eprint = {2303.06974},
  primaryclass = {astro-ph},
  publisher = {{arXiv}},
  urldate = {2023-03-16},
  abstract = {Type Ia Supernovae (SNe Ia) are considered the most reliable standard candles and they have played an invaluable role in cosmology since the discovery of the Universe's accelerated expansion. During the last decades, the SNe Ia samples have been improved in number, redshift coverage, calibration methodology, and systematics treatment. These efforts led to the most recent ``Pantheon" (2018) and ``Pantheon +" (2022) releases, which enable to constrain cosmological parameters more precisely than previous samples. In this era of precision cosmology, the community strives to find new ways to reduce uncertainties on cosmological parameters. To this end, we start our investigation even from the likelihood assumption of Gaussianity, implicitly used in this domain. Indeed, the usual practise involves constraining parameters through a Gaussian distance moduli likelihood. This method relies on the implicit assumption that the difference between the distance moduli measured and the ones expected from the cosmological model is Gaussianly distributed. In this work, we test this hypothesis for both the Pantheon and Pantheon + releases. We find that in both cases this requirement is not fulfilled and the actual underlying distributions are a logistic and a Student's t distribution for the Pantheon and Pantheon + data, respectively. When we apply these new likelihoods fitting a flat {$\Lambda$}CDM model, we significantly reduce the uncertainties on {$\Omega\mathsl{M}$} and {$\mathsl{H}$}0 of {$\sim$} 40\%. This boosts the SNe Ia power in constraining cosmological parameters, thus representing a huge step forward to shed light on the current debated tensions in cosmology.},
  archiveprefix = {arxiv},
  langid = {english},
  keywords = {Astrophysics - Cosmology and Nongalactic Astrophysics,Astrophysics - High Energy Astrophysical Phenomena},
  file = {/home/jacob/snap/zotero-snap/common/Zotero/storage/4I4M5RCH/Dainotti et al. - 2023 - Reduced uncertainties up to 43% on the Hubble con.pdf}
}

@misc{DataPreviewSchema,
  title = {Data {{Preview}} 0.1 {{Schema}} | Sdm\_schemas},
  urldate = {2022-02-21},
  howpublished = {https://dm.lsst.org/sdm\_schemas/browser/dp01.html},
  file = {/home/jacob/snap/zotero-snap/common/Zotero/storage/LQ6UHPBH/dp01.html}
}

@article{dilokthanakulDeepUnsupervisedClustering2017,
  title = {Deep {{Unsupervised Clustering}} with {{Gaussian Mixture Variational Autoencoders}}},
  author = {Dilokthanakul, Nat and Mediano, Pedro A. M. and Garnelo, Marta and Lee, Matthew C. H. and Salimbeni, Hugh and Arulkumaran, Kai and Shanahan, Murray},
  year = {2017},
  month = jan,
  journal = {arXiv:1611.02648 [cs, stat]},
  eprint = {1611.02648},
  primaryclass = {cs, stat},
  urldate = {2022-03-08},
  abstract = {We study a variant of the variational autoencoder model (VAE) with a Gaussian mixture as a prior distribution, with the goal of performing unsupervised clustering through deep generative models. We observe that the known problem of over-regularisation that has been shown to arise in regular VAEs also manifests itself in our model and leads to cluster degeneracy. We show that a heuristic called minimum information constraint that has been shown to mitigate this effect in VAEs can also be applied to improve unsupervised clustering performance with our model. Furthermore we analyse the effect of this heuristic and provide an intuition of the various processes with the help of visualizations. Finally, we demonstrate the performance of our model on synthetic data, MNIST and SVHN, showing that the obtained clusters are distinct, interpretable and result in achieving competitive performance on unsupervised clustering to the state-of-the-art results.},
  archiveprefix = {arxiv},
  langid = {english},
  keywords = {Computer Science - Machine Learning,Computer Science - Neural and Evolutionary Computing,Statistics - Machine Learning},
  file = {/home/jacob/snap/zotero-snap/common/Zotero/storage/SKGJ7WXL/Dilokthanakul et al. - 2017 - Deep Unsupervised Clustering with Gaussian Mixture.pdf}
}

@misc{dupontLearningDisentangledJoint2018,
  title = {Learning {{Disentangled Joint Continuous}} and {{Discrete Representations}}},
  author = {Dupont, Emilien},
  year = {2018},
  month = oct,
  number = {arXiv:1804.00104},
  eprint = {1804.00104},
  primaryclass = {cs, stat},
  publisher = {{arXiv}},
  urldate = {2023-10-04},
  abstract = {We present a framework for learning disentangled and interpretable jointly continuous and discrete representations in an unsupervised manner. By augmenting the continuous latent distribution of variational autoencoders with a relaxed discrete distribution and controlling the amount of information encoded in each latent unit, we show how continuous and categorical factors of variation can be discovered automatically from data. Experiments show that the framework disentangles continuous and discrete generative factors on various datasets and outperforms current disentangling methods when a discrete generative factor is prominent.},
  archiveprefix = {arxiv},
  langid = {english},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {/home/jacob/snap/zotero-snap/common/Zotero/storage/9AMLNK9V/Dupont - 2018 - Learning Disentangled Joint Continuous and Discret.pdf}
}

@misc{garcia-garciaCombiningCosmicShear2022,
  title = {Combining Cosmic Shear Data with Correlated Photo-\$z\$ Uncertainties: Constraints from {{DESY1}} and {{HSC-DR1}}},
  shorttitle = {Combining Cosmic Shear Data with Correlated Photo-\$z\$ Uncertainties},
  author = {{Garc{\'i}a-Garc{\'i}a}, Carlos and Alonso, David and Ferreira, Pedro G. and Hadzhiyska, Boryana and Nicola, Andrina and S{\'a}nchez, Carles and Slosar, An{\v z}e},
  year = {2022},
  month = oct,
  number = {arXiv:2210.13434},
  eprint = {2210.13434},
  primaryclass = {astro-ph},
  publisher = {{arXiv}},
  urldate = {2022-10-25},
  abstract = {An accurate calibration of the source redshift distribution \$p(z)\$ is a key aspect in the analysis of cosmic shear data. This, one way or another, requires the use of spectroscopic or high-quality photometric samples. However, the difficulty to obtain colour-complete spectroscopic samples matching the depth of weak lensing catalogs means that the analyses of different cosmic shear datasets often use the same samples for redshift calibration. This introduces a source of statistical and systematic uncertainty that is highly correlated across different weak lensing datasets, and which must be accurately characterised and propagated in order to obtain robust cosmological constraints from their combination. In this paper we introduce a method to quantify and propagate the uncertainties on the source redshift distribution in two different surveys sharing the same calibrating sample. The method is based on an approximate analytical marginalisation of the \$p(z)\$ statistical uncertainties and the correlated marginalisation of residual systematics. We apply this method to the combined analysis of cosmic shear data from the DESY1 data release and the HSC-DR1 data, using the COSMOS 30-band catalog as a common redshift calibration sample. We find that, although there is significant correlation in the uncertainties on the redshift distributions of both samples, this does not change the final constraints on cosmological parameters significantly. The same is true also for the impact of residual systematic uncertainties from the errors in the COSMOS 30-band photometric redshifts. Additionally, we show that these effects will still be negligible in Stage-IV datasets. Finally, the combination of DESY1 and HSC-DR1 allows us to constrain the ``clumpiness'' parameter to \$S\_8 = 0.768\^\{+0.021\}\_\{-0.017\}\$. This corresponds to a \$\textbackslash sim\textbackslash sqrt\{2\}\$ improvement in uncertainties with respect to either DES or HSC alone.},
  archiveprefix = {arxiv},
  langid = {english},
  keywords = {Astrophysics - Cosmology and Nongalactic Astrophysics},
  file = {/home/jacob/snap/zotero-snap/common/Zotero/storage/7VY2UZ2S/García-García et al. - 2022 - Combining cosmic shear data with correlated photo-.pdf}
}

@misc{GaussianMixtureVAE,
  title = {Gaussian {{Mixture VAE}}: {{Lessons}} in {{Variational Inference}}, {{Generative Models}}, and {{Deep Nets}} - {{Rui Shu}}},
  shorttitle = {Gaussian {{Mixture VAE}}},
  urldate = {2022-03-08},
  abstract = {\textbackslash [\textbackslash newcommand\{\textbackslash E\}\{\textbackslash mathbb\{E\}\}\textbackslash newcommand\{\textbackslash brac\}[1]\{\textbackslash left[\#1\textbackslash right]\}\textbackslash newcommand\{\textbackslash paren\}[1]\{\textbackslash left(\#1\textbackslash right)\}\textbackslash ]Not too l...},
  howpublished = {http://ruishu.io/2016/12/25/gmvae/},
  file = {/home/jacob/snap/zotero-snap/common/Zotero/storage/GS6HZHQ5/gmvae.html}
}

@article{girinDynamicalVariationalAutoencoders2021,
  title = {Dynamical {{Variational Autoencoders}}: {{A Comprehensive Review}}},
  shorttitle = {Dynamical {{Variational Autoencoders}}},
  author = {Girin, Laurent and Leglaive, Simon and Bie, Xiaoyu and Diard, Julien and Hueber, Thomas and {Alameda-Pineda}, Xavier},
  year = {2021},
  journal = {Foundations and Trends\textregistered{} in Machine Learning},
  volume = {15},
  number = {1-2},
  pages = {1--175},
  issn = {1935-8237, 1935-8245},
  doi = {10.1561/2200000089},
  urldate = {2023-10-04},
  langid = {english},
  file = {/home/jacob/snap/zotero-snap/common/Zotero/storage/XK27F5U5/Girin et al. - 2021 - Dynamical Variational Autoencoders A Comprehensiv.pdf}
}

@article{grahamPhotometricRedshiftsLSST2018,
  title = {Photometric {{Redshifts}} with the {{LSST}}: {{Evaluating Survey Observing Strategies}}},
  shorttitle = {Photometric {{Redshifts}} with the {{LSST}}},
  author = {Graham, Melissa L. and Connolly, Andrew J. and Ivezi{\'c}, {\v Z}eljko and Schmidt, Samuel J. and Jones, R. Lynne and Juri{\'c}, Mario and Daniel, Scott F. and Yoachim, Peter},
  year = {2018},
  month = may,
  journal = {The Astronomical Journal},
  volume = {155},
  number = {1},
  eprint = {1706.09507},
  primaryclass = {astro-ph},
  pages = {1},
  issn = {1538-3881},
  doi = {10.3847/1538-3881/aa99d4},
  urldate = {2022-10-20},
  abstract = {In this paper we present and characterize a nearest-neighbors color-matching photometric redshift estimator that features a direct relationship between the precision and accuracy of the input magnitudes and the output photometric redshifts. This aspect makes our estimator an ideal tool for evaluating the impact of changes to LSST survey parameters that affect the measurement errors of the photometry, which is the main motivation of our work (i.e., it is not intended to provide the ``best'' photometric redshifts for LSST data). We show how the photometric redshifts will improve with time over the 10-year LSST survey and confirm that the nominal distribution of visits per filter provides the most accurate photo-z results. The LSST survey strategy naturally produces observations over a range of airmass, which offers the opportunity of using an SED- and z-dependent atmospheric affect on the observed photometry as a color-independent redshift indicator. We show that measuring this airmass effect and including it as a prior has the potential to improve the photometric redshifts and can ameliorate extreme outliers, but that it will only be adequately measured for the brightest galaxies, which limits its overall impact on LSST photometric redshifts. We furthermore demonstrate how this airmass effect can induce a bias in the photo-z results, and caution against survey strategies that prioritize high-airmass observations for the purpose of improving this prior. Ultimately, we intend for this work to serve as a guide for the expectations and preparations of the LSST science community with regards to the minimum quality of photo-z as the survey progresses.},
  archiveprefix = {arxiv},
  langid = {english},
  keywords = {Astrophysics - Astrophysics of Galaxies,Astrophysics - Cosmology and Nongalactic Astrophysics},
  file = {/home/jacob/snap/zotero-snap/common/Zotero/storage/H6D9242E/Graham et al. - 2018 - Photometric Redshifts with the LSST Evaluating Su.pdf}
}

@misc{HowTrustYour2020,
  title = {How to {{Trust Your Deep Learning Code}}},
  year = {2020},
  month = aug,
  urldate = {2022-10-12},
  abstract = {Deep learning is a discipline where the correctness of code is hard to assess. Random initialization, huge datasets and limited interpretability of weights mean that finding the exact issue of why your model is not training, is trial-and-error most times. In classical software development, automated unit tests are the bread and butter for determining if your code does what it is supposed to do. It helps the developer to trust their code and be confident when introducing changes.},
  chapter = {posts},
  howpublished = {https://tilman151.github.io/posts/deep-learning-unit-tests/},
  langid = {english},
  file = {/home/jacob/snap/zotero-snap/common/Zotero/storage/ZCEJBB6D/deep-learning-unit-tests.html}
}

@article{humphreyImprovingMachineLearningderived2023a,
  title = {Improving Machine Learning-Derived Photometric Redshifts and Physical Property Estimates Using Unlabelled Observations},
  author = {Humphrey, A and Cunha, P A C and {Paulino-Afonso}, A and Amarantidis, S and Carvajal, R and Gomes, J M and Matute, I and Papaderos, P},
  year = {2023},
  month = jan,
  journal = {Monthly Notices of the Royal Astronomical Society},
  volume = {520},
  number = {1},
  pages = {305--313},
  issn = {0035-8711, 1365-2966},
  doi = {10.1093/mnras/stac3596},
  urldate = {2023-09-13},
  abstract = {ABSTRACT             In the era of huge astronomical surveys, machine learning offers promising solutions for the efficient estimation of galaxy properties. The traditional, `supervised' paradigm for the application of machine learning involves training a model on labelled data, and using this model to predict the labels of previously unlabelled data. The semi-supervised `pseudo-labelling' technique offers an alternative paradigm, allowing the model training algorithm to learn from both labelled data and as-yet unlabelled data. We test the pseudo-labelling method on the problems of estimating redshift, stellar mass, and star formation rate, using COSMOS2015 broad band photometry and one of several publicly available machine learning algorithms, and we obtain significant improvements compared to purely supervised learning. We find that the gradient-boosting tree methods CatBoost, XGBoost, and LightGBM benefit the most, with reductions of up to {$\sim$}15\,~per~cent in metrics of absolute error. We also find similar improvements in the photometric redshift catastrophic outlier fraction. We argue that the pseudo-labelling technique will be useful for the estimation of redshift and physical properties of galaxies in upcoming large imaging surveys such as Euclid and LSST, which will provide photometric data for billions of sources.},
  langid = {english},
  file = {/home/jacob/snap/zotero-snap/common/Zotero/storage/QMFFIJLD/Humphrey et al. - 2023 - Improving machine learning-derived photometric red.pdf}
}

@article{jiMultiModalAnomalyDetection2020,
  title = {Multi-{{Modal Anomaly Detection}} for {{Unstructured}} and {{Uncertain Environments}}},
  author = {Ji, Tianchen and Vuppala, Sri Theja and Chowdhary, Girish and {Driggs-Campbell}, Katherine},
  year = {2020},
  month = dec,
  journal = {arXiv:2012.08637 [cs]},
  eprint = {2012.08637},
  primaryclass = {cs},
  urldate = {2022-03-29},
  abstract = {To achieve high-levels of autonomy, modern robots require the ability to detect and recover from anomalies and failures with minimal human supervision. Multi-modal sensor signals could provide more information for such anomaly detection tasks; however, the fusion of high-dimensional and heterogeneous sensor modalities remains a challenging problem. We propose a deep learning neural network: supervised variational autoencoder (SVAE), for failure identification in unstructured and uncertain environments. Our model leverages the representational power of VAE to extract robust features from high-dimensional inputs for supervised learning tasks. The training objective unifies the generative model and the discriminative model, thus making the learning a one-stage procedure. Our experiments on real field robot data demonstrate superior failure identification performance than baseline methods, and that our model learns interpretable representations. Videos of our results are available on our website: https://sites.google.com/illinois.edu/supervised-vae.},
  archiveprefix = {arxiv},
  langid = {english},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning,Computer Science - Robotics},
  file = {/home/jacob/snap/zotero-snap/common/Zotero/storage/JPXPWGBM/Ji et al. - 2020 - Multi-Modal Anomaly Detection for Unstructured and.pdf}
}

@misc{jonesPhotometricRedshiftsCosmology2023,
  title = {Photometric {{Redshifts}} for {{Cosmology}}: {{Improving Accuracy}} and {{Uncertainty Estimates Using Bayesian Neural Networks}}},
  shorttitle = {Photometric {{Redshifts}} for {{Cosmology}}},
  author = {Jones, Evan and Do, Tuan and Boscoe, Bernie and Singal, Jack and Wan, Yujie and Nguyen, Zooey},
  year = {2023},
  month = jun,
  number = {arXiv:2306.13179},
  eprint = {2306.13179},
  primaryclass = {astro-ph},
  publisher = {{arXiv}},
  urldate = {2023-06-27},
  abstract = {We present results exploring the role that probabilistic deep learning models play in cosmology from large-scale astronomical surveys through photometric redshift (photo-z) estimation. Photo-z uncertainty estimates are critical for the science goals of upcoming large-scale surveys such as LSST, however common machine learning methods typically provide only point estimates and lack uncertainties on predictions. We turn to Bayesian neural networks (BNNs) as a promising way to provide accurate predictions of redshift values with uncertainty estimates. We have compiled a new galaxy training data set from the Hyper Suprime-Cam Survey with grizy photometry, which is designed to be a smaller scale version of large surveys like LSST. We use this data set to investigate the performance of a neural network (NN) and a probabilistic BNN for photo-z estimation and evaluate their performance with respect to LSST photo-z science requirements. We also examine the utility of photo-z uncertainties as a means to reduce catastrophic outlier estimates. The BNN model outputs the estimate in the form of a Gaussian probability distribution. We use the mean and standard deviation as the redshift estimate and uncertainty, respectively. We find that the BNN can produce accurate uncertainties. Using a coverage test, we find excellent agreement with expectation \textendash{} 67.2\% of galaxies between 0 {$<$} 2.5 have 1-{$\sigma$} uncertainties that cover the spectroscopic value. We find the BNN meets 2/3 of the LSST photo-z science requirements in the range 0 {$<$} z {$<$} 2.5 and generally outperforms the alternative photo-z methods considered here on the same data.},
  archiveprefix = {arxiv},
  langid = {english},
  keywords = {Astrophysics - Cosmology and Nongalactic Astrophysics},
  file = {/home/jacob/snap/zotero-snap/common/Zotero/storage/FYY88A9N/Jones et al. - 2023 - Photometric Redshifts for Cosmology Improving Acc.pdf}
}

@misc{joyCapturingLabelCharacteristics2022,
  title = {Capturing {{Label Characteristics}} in {{VAEs}}},
  author = {Joy, Tom and Schmon, Sebastian M. and Torr, Philip H. S. and Siddharth, N. and Rainforth, Tom},
  year = {2022},
  month = dec,
  number = {arXiv:2006.10102},
  eprint = {2006.10102},
  primaryclass = {cs, stat},
  publisher = {{arXiv}},
  urldate = {2023-10-04},
  abstract = {We present a principled approach to incorporating labels in variational autoencoders (VAEs) that captures the rich characteristic information associated with those labels. While prior work has typically conflated these by learning latent variables that directly correspond to label values, we argue this is contrary to the intended effect of supervision in VAEs\textemdash capturing rich label characteristics with the latents. For example, we may want to capture the characteristics of a face that make it look young, rather than just the age of the person. To this end, we develop the characteristic capturing VAE (CCVAE), a novel VAE model and concomitant variational objective which captures label characteristics explicitly in the latent space, eschewing direct correspondences between label values and latents. Through judicious structuring of mappings between such characteristic latents and labels, we show that the CCVAE can effectively learn meaningful representations of the characteristics of interest across a variety of supervision schemes. In particular, we show that the CCVAE allows for more effective and more general interventions to be performed, such as smooth traversals within the characteristics for a given label, diverse conditional generation, and transferring characteristics across datapoints1.},
  archiveprefix = {arxiv},
  langid = {english},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {/home/jacob/snap/zotero-snap/common/Zotero/storage/KBGY9GAE/Joy et al. - 2022 - Capturing Label Characteristics in VAEs.pdf}
}

@misc{kengSemisupervisedLearningVariational2017,
  title = {Semi-Supervised {{Learning}} with {{Variational Autoencoders}}},
  author = {Keng, Brian},
  year = {2017},
  month = sep,
  journal = {Bounded Rationality},
  urldate = {2022-05-05},
  abstract = {A post on semi-supervised learning with variational autoencoders.},
  howpublished = {http://bjlkeng.github.io/posts/semi-supervised-learning-with-variational-autoencoders/},
  langid = {english},
  file = {/home/jacob/snap/zotero-snap/common/Zotero/storage/CKA6Z9DQ/semi-supervised-learning-with-variational-autoencoders.html}
}

@misc{kingmaAdamMethodStochastic2017,
  title = {Adam: {{A Method}} for {{Stochastic Optimization}}},
  shorttitle = {Adam},
  author = {Kingma, Diederik P. and Ba, Jimmy},
  year = {2017},
  month = jan,
  number = {arXiv:1412.6980},
  eprint = {1412.6980},
  primaryclass = {cs},
  publisher = {{arXiv}},
  urldate = {2023-10-15},
  abstract = {We introduce Adam, an algorithm for first-order gradient-based optimization of stochastic objective functions, based on adaptive estimates of lower-order moments. The method is straightforward to implement, is computationally efficient, has little memory requirements, is invariant to diagonal rescaling of the gradients, and is well suited for problems that are large in terms of data and/or parameters. The method is also appropriate for non-stationary objectives and problems with very noisy and/or sparse gradients. The hyper-parameters have intuitive interpretations and typically require little tuning. Some connections to related algorithms, on which Adam was inspired, are discussed. We also analyze the theoretical convergence properties of the algorithm and provide a regret bound on the convergence rate that is comparable to the best known results under the online convex optimization framework. Empirical results demonstrate that Adam works well in practice and compares favorably to other stochastic optimization methods. Finally, we discuss AdaMax, a variant of Adam based on the infinity norm.},
  archiveprefix = {arxiv},
  langid = {english},
  keywords = {Computer Science - Machine Learning},
  file = {/home/jacob/snap/zotero-snap/common/Zotero/storage/YICA22XJ/Kingma and Ba - 2017 - Adam A Method for Stochastic Optimization.pdf}
}

@misc{kingmaAutoEncodingVariationalBayes2022,
  title = {Auto-{{Encoding Variational Bayes}}},
  author = {Kingma, Diederik P. and Welling, Max},
  year = {2022},
  month = dec,
  number = {arXiv:1312.6114},
  eprint = {1312.6114},
  primaryclass = {cs, stat},
  publisher = {{arXiv}},
  urldate = {2023-10-04},
  abstract = {How can we perform efficient inference and learning in directed probabilistic models, in the presence of continuous latent variables with intractable posterior distributions, and large datasets? We introduce a stochastic variational inference and learning algorithm that scales to large datasets and, under some mild differentiability conditions, even works in the intractable case. Our contributions are two-fold. First, we show that a reparameterization of the variational lower bound yields a lower bound estimator that can be straightforwardly optimized using standard stochastic gradient methods. Second, we show that for i.i.d. datasets with continuous latent variables per datapoint, posterior inference can be made especially efficient by fitting an approximate inference model (also called a recognition model) to the intractable posterior using the proposed lower bound estimator. Theoretical advantages are reflected in experimental results.},
  archiveprefix = {arxiv},
  langid = {english},
  keywords = {Bayesian Inference,Computer Science - Machine Learning,Statistics - Machine Learning,VAEs,Variational Inference},
  file = {/home/jacob/snap/zotero-snap/common/Zotero/storage/CT3L8UXI/Kingma and Welling - 2022 - Auto-Encoding Variational Bayes.pdf}
}

@article{kingmaIntroductionVariationalAutoencoders2019,
  title = {An {{Introduction}} to {{Variational Autoencoders}}},
  author = {Kingma, Diederik P. and Welling, Max},
  year = {2019},
  journal = {Foundations and Trends\textregistered{} in Machine Learning},
  volume = {12},
  number = {4},
  eprint = {1906.02691},
  primaryclass = {cs, stat},
  pages = {307--392},
  issn = {1935-8237, 1935-8245},
  doi = {10.1561/2200000056},
  urldate = {2023-10-04},
  abstract = {Variational autoencoders provide a principled framework for learning deep latent-variable models and corresponding inference models. In this work, we provide an introduction to variational autoencoders and some important extensions.},
  archiveprefix = {arxiv},
  langid = {english},
  keywords = {Bayesian Inference,Computer Science - Machine Learning,Statistics - Machine Learning,VAEs,Variational Inference},
  file = {/home/jacob/snap/zotero-snap/common/Zotero/storage/UJ7YGNM2/Kingma and Welling - 2019 - An Introduction to Variational Autoencoders.pdf}
}

@misc{kingmaSemiSupervisedLearningDeep2014,
  title = {Semi-{{Supervised Learning}} with {{Deep Generative Models}}},
  author = {Kingma, Diederik P. and Rezende, Danilo J. and Mohamed, Shakir and Welling, Max},
  year = {2014},
  month = oct,
  number = {arXiv:1406.5298},
  eprint = {1406.5298},
  primaryclass = {cs, stat},
  publisher = {{arXiv}},
  urldate = {2022-10-18},
  abstract = {The ever-increasing size of modern data sets combined with the difficulty of obtaining label information has made semi-supervised learning one of the problems of significant practical importance in modern data analysis. We revisit the approach to semi-supervised learning with generative models and develop new models that allow for effective generalisation from small labelled data sets to large unlabelled ones. Generative approaches have thus far been either inflexible, inefficient or non-scalable. We show that deep generative models and approximate Bayesian inference exploiting recent advances in variational methods can be used to provide significant improvements, making generative approaches highly competitive for semi-supervised learning.},
  archiveprefix = {arxiv},
  langid = {english},
  keywords = {Bayesian Inference,Computer Science - Machine Learning,Semi-Supervised Learning,Statistics - Machine Learning,VAEs,Variational Inference},
  file = {/home/jacob/snap/zotero-snap/common/Zotero/storage/SM8GQ2SI/Kingma et al. - 2014 - Semi-Supervised Learning with Deep Generative Mode.pdf}
}

@misc{kodraOptimizedPhotometricRedshifts2022,
  title = {Optimized {{Photometric Redshifts}} for the {{Cosmic Assembly Near-Infrared Deep Extragalactic Legacy Survey}} ({{CANDELS}})},
  author = {Kodra, Dritan and Andrews, Brett H. and Newman, Jeffrey A. and Finkelstein, Steven L. and Fontana, Adriano and Hathi, Nimish and Salvato, Mara and Wiklind, Tommy and Wuyts, Stijn and Broussard, Adam and Chartab, Nima and Conselice, Christopher and Cooper, M. C. and Dekel, Avishai and Dickinson, Mark and Ferguson, Harry and Gawiser, Eric and Grogin, Norman A. and Iyer, Kartheik and Kartaltepe, Jeyhan and Kassin, Susan and Koekemoer, Anton M. and Koo, David C. and Lucas, Ray A. and Mantha, Kameswara Bharadwaj and McIntosh, Daniel H. and Mobasher, Bahram and Pacifici, Camilla and {P{\'e}rez-Gonz{\'a}lez}, Pablo G. and Santini, Paola},
  year = {2022},
  month = oct,
  number = {arXiv:2210.01140},
  eprint = {2210.01140},
  primaryclass = {astro-ph},
  publisher = {{arXiv}},
  urldate = {2022-10-12},
  abstract = {We present the first comprehensive release of photometric redshifts (photo-z's) from the Cosmic Assembly Near-Infrared Deep Extragalactic Legacy Survey (CANDELS) team. We use statistics based upon the Quantile-Quantile (Q--Q) plot to identify biases and signatures of underestimated or overestimated errors in photo-z probability density functions (PDFs) produced by six groups in the collaboration; correcting for these effects makes the resulting PDFs better match the statistical definition of a PDF. After correcting each group's PDF, we explore three methods of combining the different groups' PDFs for a given object into a consensus curve. Two of these methods are based on identifying the minimum f-divergence curve, i.e., the PDF that is closest in aggregate to the other PDFs in a set (analogous to the median of an array of numbers). We demonstrate that these techniques yield improved results using sets of spectroscopic redshifts independent of those used to optimize PDF modifications. The best photo-z PDFs and point estimates are achieved with the minimum f-divergence using the best 4 PDFs for each object (mFDa4) and the Hierarchical Bayesian (HB4) methods, respectively. The HB4 photo-z point estimates produced \$\textbackslash sigma\_\{\textbackslash rm NMAD\} = 0.0227/0.0189\$ and \$|\textbackslash Delta z/(1+z)| {$>$} 0.15\$ outlier fraction = 0.067/0.019 for spectroscopic and 3D-HST redshifts, respectively. Finally, we describe the structure and provide guidance for the use of the CANDELS photo-z catalogs, which are available at https://archive.stsci.edu/hlsp/candels.},
  archiveprefix = {arxiv},
  langid = {english},
  keywords = {Astrophysics - Astrophysics of Galaxies},
  file = {/home/jacob/snap/zotero-snap/common/Zotero/storage/6VD3VLD7/Kodra et al. - 2022 - Optimized Photometric Redshifts for the Cosmic Ass.pdf}
}

@article{korytovCosmoDC2SyntheticSky2019,
  title = {{{CosmoDC2}}: {{A Synthetic Sky Catalog}} for {{Dark Energy Science}} with {{LSST}}},
  shorttitle = {{{CosmoDC2}}},
  author = {Korytov, Danila and Hearin, Andrew and Kovacs, Eve and Larsen, Patricia and Rangel, Esteban and Hollowed, Joseph and Benson, Andrew J. and Heitmann, Katrin and Mao, Yao-Yuan and Bahmanyar, Anita and Chang, Chihway and Campbell, Duncan and DeRose, Joseph and Finkel, Hal and Frontiere, Nicholas and Gawiser, Eric and Habib, Salman and Joachimi, Benjamin and Lanusse, Fran{\c c}ois and Li, Nan and Mandelbaum, Rachel and Morrison, Christopher and Newman, Jeffrey A. and Pope, Adrian and Rykoff, Eli and Simet, Melanie and To, Chun-Hao and Vikraman, Vinu and Wechsler, Risa H. and White, Martin and {(The LSST Dark Energy Science Collaboration)}},
  year = {2019},
  month = dec,
  journal = {The Astrophysical Journal Supplement Series},
  volume = {245},
  number = {2},
  pages = {26},
  issn = {1538-4365},
  doi = {10.3847/1538-4365/ab510c},
  urldate = {2022-02-21},
  abstract = {This paper introduces cosmoDC2, a large synthetic galaxy catalog designed to support precision dark energy science with the Large Synoptic Survey Telescope (LSST). CosmoDC2 is the starting point for the second data challenge (DC2) carried out by the LSST Dark Energy Science Collaboration (LSST DESC). The catalog is based on a trillion-particle, (4.225 Gpc)3 box cosmological N-body simulation, the Outer Rim run. It covers 440deg2 of sky area to a redshift of z=3 and matches expected number densities from contemporary surveys to a magnitude depth of 28 in the r band. Each galaxy is characterized by a multitude of galaxy properties including stellar mass, morphology, spectral energy distributions, broadband filter magnitudes, host halo information, and weak lensing shear. The size and complexity of cosmoDC2 requires an efficient catalog generation methodology; our approach is based on a new hybrid technique that combines data-based empirical approaches with semianalytic galaxy modeling. A wide range of observation-based validation tests has been implemented to ensure that cosmoDC2 enables the science goals of the planned LSSTDESC DC2 analyses. This paper also represents the official release of the cosmoDC2 data set, including an efficient reader that facilitates interaction with the data.},
  langid = {english},
  file = {/home/jacob/snap/zotero-snap/common/Zotero/storage/ADWWVT9U/Korytov et al. - 2019 - CosmoDC2 A Synthetic Sky Catalog for Dark Energy .pdf}
}

@article{leistedtDatadrivenInterpretablePhotometric2017,
  title = {Data-Driven, {{Interpretable Photometric Redshifts Trained}} on {{Heterogeneous}} and {{Unrepresentative Data}}},
  author = {Leistedt, Boris and Hogg, David W.},
  year = {2017},
  month = mar,
  journal = {The Astrophysical Journal},
  volume = {838},
  number = {1},
  pages = {5},
  issn = {1538-4357},
  doi = {10.3847/1538-4357/aa6332},
  urldate = {2023-09-13},
  abstract = {We present a new method for inferring photometric redshifts in deep galaxy and quasar surveys, based on a datadriven model of latent spectral energy distributions (SEDs) and a physical model of photometric fluxes as a function of redshift. This conceptually novel approach combines the advantages of both machine learning methods and template fitting methods by building template SEDs directly from the spectroscopic training data. This is made computationally tractable with Gaussian processes operating in flux\textendash redshift space, encoding the physics of redshifts and the projection of galaxy SEDs onto photometric bandpasses. This method alleviates the need to acquire representative training data or to construct detailed galaxy SED models; it requires only that the photometric bandpasses and calibrations be known or have parameterized unknowns. The training data can consist of a combination of spectroscopic and deep many-band photometric data with reliable redshifts, which do not need to entirely spatially overlap with the target survey of interest or even involve the same photometric bands. We showcase the method on the i-magnitude-selected, spectroscopically confirmed galaxies in the COSMOS field. The model is trained on the deepest bands (from SUBARU and HST) and photometric redshifts are derived using the shallower SDSS optical bands only. We demonstrate that we obtain accurate redshift point estimates and probability distributions despite the training and target sets having very different redshift distributions, noise properties, and even photometric bands. Our model can also be used to predict missing photometric fluxes or to simulate populations of galaxies with realistic fluxes and redshifts, for example.},
  langid = {english},
  file = {/home/jacob/snap/zotero-snap/common/Zotero/storage/6MXX5WIY/Leistedt and Hogg - 2017 - Data-driven, Interpretable Photometric Redshifts T.pdf}
}

@article{lsstdarkenergysciencecollaborationLSSTDESCDC22021,
  title = {The {{LSST DESC DC2 Simulated Sky Survey}}},
  author = {LSST Dark Energy Science Collaboration and Abolfathi, Bela and Alonso, David and Armstrong, Robert and Aubourg, {\'E}ric and Awan, Humna and Babuji, Yadu N. and Bauer, Franz Erik and Bean, Rachel and Beckett, George and Biswas, Rahul and Bogart, Joanne R. and Boutigny, Dominique and Chard, Kyle and Chiang, James and Claver, Chuck F. and {Cohen-Tanugi}, Johann and Combet, C{\'e}line and Connolly, Andrew J. and Daniel, Scott F. and Digel, Seth W. and {Drlica-Wagner}, Alex and Dubois, Richard and Gangler, Emmanuel and Gawiser, Eric and Glanzman, Thomas and Gris, Phillipe and Habib, Salman and Hearin, Andrew P. and Heitmann, Katrin and Hernandez, Fabio and Hlo{\v z}ek, Ren{\'e}e and Hollowed, Joseph and Ishak, Mustapha and Ivezi{\'c}, {\v Z}eljko and Jarvis, Mike and Jha, Saurabh W. and Kahn, Steven M. and Kalmbach, J. Bryce and Kelly, Heather M. and Kovacs, Eve and Korytov, Danila and Krughoff, K. Simon and Lage, Craig S. and Lanusse, Fran{\c c}ois and Larsen, Patricia and Guillou, Laurent Le and Li, Nan and Longley, Emily Phillips and Lupton, Robert H. and Mandelbaum, Rachel and Mao, Yao-Yuan and Marshall, Phil and Meyers, Joshua E. and Moniez, Marc and Morrison, Christopher B. and Nomerotski, Andrei and O'Connor, Paul and Park, HyeYun and Park, Ji Won and Peloton, Julien and Perrefort, Daniel and Perry, James and Plaszczynski, St{\'e}phane and Pope, Adrian and Rasmussen, Andrew and Reil, Kevin and Roodman, Aaron J. and Rykoff, Eli S. and S{\'a}nchez, F. Javier and Schmidt, Samuel J. and Scolnic, Daniel and Stubbs, Christopher W. and Tyson, J. Anthony and Uram, Thomas D. and Villarreal, Antonio and Walter, Christopher W. and Wiesner, Matthew P. and {Wood-Vasey}, W. Michael and Zuntz, Joe},
  year = {2021},
  month = mar,
  journal = {The Astrophysical Journal Supplement Series},
  volume = {253},
  number = {1},
  eprint = {2010.05926},
  pages = {31},
  issn = {0067-0049, 1538-4365},
  doi = {10.3847/1538-4365/abd62c},
  urldate = {2022-02-21},
  abstract = {We describe the simulated sky survey underlying the second data challenge (DC2) carried out in preparation for analysis of the Vera C. Rubin Observatory Legacy Survey of Space and Time (LSST) by the LSST Dark Energy Science Collaboration (LSST DESC). Significant connections across multiple science domains will be a hallmark of LSST; the DC2 program represents a unique modeling effort that stresses this interconnectivity in a way that has not been attempted before. This effort encompasses a full end-to-end approach: starting from a large N-body simulation, through setting up LSST-like observations including realistic cadences, through image simulations, and finally processing with Rubin's LSST Science Pipelines. This last step ensures that we generate data products resembling those to be delivered by the Rubin Observatory as closely as is currently possible. The simulated DC2 sky survey covers six optical bands in a wide-fast-deep (WFD) area of approximately 300 deg2 as well as a deep drilling field (DDF) of approximately 1 deg2. We simulate 5 years of the planned 10-year survey. The DC2 sky survey has multiple purposes. First, the LSST DESC working groups can use the dataset to develop a range of DESC analysis pipelines to prepare for the advent of actual data. Second, it serves as a realistic testbed for the image processing software under development for LSST by the Rubin Observatory. In particular, simulated data provide a controlled way to investigate certain image-level systematic effects. Finally, the DC2 sky survey enables the exploration of new scientific ideas in both static and time-domain cosmology.},
  archiveprefix = {arxiv},
  langid = {english},
  keywords = {Astrophysics - Cosmology and Nongalactic Astrophysics,Astrophysics - Instrumentation and Methods for Astrophysics},
  file = {/home/jacob/snap/zotero-snap/common/Zotero/storage/YDBDFJGU/LSST Dark Energy Science Collaboration et al. - 2021 - The LSST DESC DC2 Simulated Sky Survey.pdf}
}

@misc{maaloeAuxiliaryDeepGenerative2016,
  title = {Auxiliary {{Deep Generative Models}}},
  author = {Maal{\o}e, Lars and S{\o}nderby, Casper Kaae and S{\o}nderby, S{\o}ren Kaae and Winther, Ole},
  year = {2016},
  month = jun,
  number = {arXiv:1602.05473},
  eprint = {1602.05473},
  primaryclass = {cs, stat},
  publisher = {{arXiv}},
  urldate = {2023-10-04},
  abstract = {Deep generative models parameterized by neural networks have recently achieved state-ofthe-art performance in unsupervised and semisupervised learning. We extend deep generative models with auxiliary variables which improves the variational approximation. The auxiliary variables leave the generative model unchanged but make the variational distribution more expressive. Inspired by the structure of the auxiliary variable we also propose a model with two stochastic layers and skip connections. Our findings suggest that more expressive and properly specified deep generative models converge faster with better results. We show state-of-theart performance within semi-supervised learning on MNIST, SVHN and NORB datasets.},
  archiveprefix = {arxiv},
  langid = {english},
  keywords = {Bayesian Inference,Computer Science - Artificial Intelligence,Computer Science - Machine Learning,Semi-Supervised Learning,Statistics - Machine Learning,VAEs,Variational Inference},
  file = {/home/jacob/snap/zotero-snap/common/Zotero/storage/VCCDS93F/Maaløe et al. - 2016 - Auxiliary Deep Generative Models.pdf}
}

@misc{maaloeBIVAVeryDeep2019,
  title = {{{BIVA}}: {{A Very Deep Hierarchy}} of {{Latent Variables}} for {{Generative Modeling}}},
  shorttitle = {{{BIVA}}},
  author = {Maal{\o}e, Lars and Fraccaro, Marco and Li{\'e}vin, Valentin and Winther, Ole},
  year = {2019},
  month = nov,
  number = {arXiv:1902.02102},
  eprint = {1902.02102},
  primaryclass = {cs, stat},
  publisher = {{arXiv}},
  urldate = {2023-10-11},
  abstract = {With the introduction of the variational autoencoder (VAE), probabilistic latent variable models have received renewed attention as powerful generative models. However, their performance in terms of test likelihood and quality of generated samples has been surpassed by autoregressive models without stochastic units. Furthermore, flow-based models have recently been shown to be an attractive alternative that scales well to high-dimensional data. In this paper we close the performance gap by constructing VAE models that can effectively utilize a deep hierarchy of stochastic variables and model complex covariance structures. We introduce the Bidirectional-Inference Variational Autoencoder (BIVA), characterized by a skip-connected generative model and an inference network formed by a bidirectional stochastic inference path. We show that BIVA reaches state-of-the-art test likelihoods, generates sharp and coherent natural images, and uses the hierarchy of latent variables to capture different aspects of the data distribution. We observe that BIVA, in contrast to recent results, can be used for anomaly detection. We attribute this to the hierarchy of latent variables which is able to extract high-level semantic features. Finally, we extend BIVA to semi-supervised classification tasks and show that it performs comparably to state-of-the-art results by generative adversarial networks.},
  archiveprefix = {arxiv},
  langid = {english},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {/home/jacob/snap/zotero-snap/common/Zotero/storage/AFJQL7TE/Maaløe et al. - 2019 - BIVA A Very Deep Hierarchy of Latent Variables fo.pdf}
}

@article{malzHowObtainRedshift2022,
  title = {How to {{Obtain}} the {{Redshift Distribution}} from {{Probabilistic Redshift Estimates}}},
  author = {Malz, Alex I. and Hogg, David W.},
  year = {2022},
  month = apr,
  journal = {The Astrophysical Journal},
  volume = {928},
  number = {2},
  pages = {127},
  issn = {0004-637X, 1538-4357},
  doi = {10.3847/1538-4357/ac062f},
  urldate = {2023-08-14},
  abstract = {A reliable estimate of the redshift distribution n(z) is crucial for using weak gravitational lensing and large-scale structures of galaxy catalogs to study cosmology. Spectroscopic redshifts for the dim and numerous galaxies of next-generation weak-lensing surveys are expected to be unavailable, making photometric redshift (photo-z) probability density functions (PDFs) the next best alternative for comprehensively encapsulating the nontrivial systematics affecting photo-z point estimation. The established stacked estimator of n(z) avoids reducing photo-z PDFs to point estimates but yields a systematically biased estimate of n(z) that worsens with a decreasing signal-tonoise ratio, the very regime where photo-z PDFs are most necessary. We introduce Cosmological Hierarchical Inference with Probabilistic Photometric Redshifts (CHIPPR), a statistically rigorous probabilistic graphical model of redshift-dependent photometry that correctly propagates the redshift uncertainty information beyond the best-fit estimator of n(z) produced by traditional procedures and is provably the only self-consistent way to recover n(z) from photo-z PDFs. We present the chippr prototype code, noting that the mathematically justifiable approach incurs computational cost. The CHIPPR approach is applicable to any one-point statistic of any random variable, provided the prior probability density used to produce the posteriors is explicitly known; if the prior is implicit, as may be the case for popular photo-z techniques, then the resulting posterior PDFs cannot be used for scientific inference. We therefore recommend that the photo-z community focus on developing methodologies that enable the recovery of photo-z likelihoods with support over all redshifts, either directly or via a known prior probability density.},
  langid = {english},
  file = {/home/jacob/snap/zotero-snap/common/Zotero/storage/D6E9G8CC/Malz and Hogg - 2022 - How to Obtain the Redshift Distribution from Proba.pdf}
}

@misc{mathieuDisentanglingDisentanglementVariational2019,
  title = {Disentangling {{Disentanglement}} in {{Variational Autoencoders}}},
  author = {Mathieu, Emile and Rainforth, Tom and Siddharth, N. and Teh, Yee Whye},
  year = {2019},
  month = jun,
  number = {arXiv:1812.02833},
  eprint = {1812.02833},
  primaryclass = {cs, stat},
  publisher = {{arXiv}},
  urldate = {2023-10-04},
  abstract = {We develop a generalisation of disentanglement in variational autoencoders (VAEs)\textemdash decomposition of the latent representation\textemdash characterising it as the fulfilment of two factors: a) the latent encodings of the data having an appropriate level of overlap, and b) the aggregate encoding of the data conforming to a desired structure, represented through the prior. Decomposition permits disentanglement, i.e. explicit independence between latents, as a special case, but also allows for a much richer class of properties to be imposed on the learnt representation, such as sparsity, clustering, independent subspaces, or even intricate hierarchical dependency relationships. We show that the {$\beta$}-VAE varies from the standard VAE predominantly in its control of latent overlap and that for the standard choice of an isotropic Gaussian prior, its objective is invariant to rotations of the latent representation. Viewed from the decomposition perspective, breaking this invariance with simple manipulations of the prior can yield better disentanglement with little or no detriment to reconstructions. We further demonstrate how other choices of prior can assist in producing different decompositions and introduce an alternative training objective that allows the control of both decomposition factors in a principled manner.},
  archiveprefix = {arxiv},
  langid = {english},
  keywords = {Bayesian Inference,Computer Science - Machine Learning,Disentanglement,Statistics - Machine Learning,VAEs,Variational Inference},
  file = {/home/jacob/snap/zotero-snap/common/Zotero/storage/5PXJ4PEQ/Mathieu et al. - 2019 - Disentangling Disentanglement in Variational Autoe.pdf}
}

@article{newmanPhotometricRedshiftsNextGeneration2022,
  title = {Photometric {{Redshifts}} for {{Next-Generation Surveys}}},
  author = {Newman, Jeffrey A. and Gruen, Daniel},
  year = {2022},
  month = aug,
  journal = {Annual Review of Astronomy and Astrophysics},
  volume = {60},
  number = {1},
  eprint = {2206.13633},
  primaryclass = {astro-ph},
  pages = {363--414},
  issn = {0066-4146, 1545-4282},
  doi = {10.1146/annurev-astro-032122-014611},
  urldate = {2022-10-30},
  abstract = {Photometric redshifts are essential in studies of both galaxy evolution and cosmology, as they enable analyses of objects too numerous or faint for spectroscopy. The Rubin Observatory, Euclid, and Roman Space Telescope will soon provide a new generation of imaging surveys with unprecedented area coverage, wavelength range, and depth. To take full advantage of these datasets, further progress in photometric redshift methods is needed. In this review, we focus on the greatest common challenges and prospects for improvement in applications of photo-z's to the next generation of surveys: \textbullet{} Gains in performance \textendash{} i.e., the precision of redshift estimates for individual galaxies \textendash{} could greatly enhance studies of galaxy evolution and some probes of cosmology.},
  archiveprefix = {arxiv},
  langid = {english},
  keywords = {Astrophysics - Astrophysics of Galaxies,Astrophysics - Cosmology and Nongalactic Astrophysics},
  file = {/home/jacob/snap/zotero-snap/common/Zotero/storage/4XUX5I2P/Newman and Gruen - 2022 - Photometric Redshifts for Next-Generation Surveys.pdf}
}

@article{prasadVariationalClusteringLeveraging2020,
  title = {Variational {{Clustering}}: {{Leveraging Variational Autoencoders}} for {{Image Clustering}}},
  shorttitle = {Variational {{Clustering}}},
  author = {Prasad, Vignesh and Das, Dipanjan and Bhowmick, Brojeshwar},
  year = {2020},
  month = may,
  journal = {arXiv:2005.04613 [cs]},
  eprint = {2005.04613},
  primaryclass = {cs},
  urldate = {2022-03-08},
  abstract = {Recent advances in deep learning have shown their ability to learn strong feature representations for images. The task of image clustering naturally requires good feature representations to capture the distribution of the data and subsequently differentiate data points from one another. Often these two aspects are dealt with independently and thus traditional feature learning alone does not suffice in partitioning the data meaningfully. Variational Autoencoders (VAEs) naturally lend themselves to learning data distributions in a latent space. Since we wish to efficiently discriminate between different clusters in the data, we propose a method based on VAEs where we use a Gaussian Mixture prior to help cluster the images accurately. We jointly learn the parameters of both the prior and the posterior distributions. Our method represents a true Gaussian Mixture VAE. This way, our method simultaneously learns a prior that captures the latent distribution of the images and a posterior to help discriminate well between data points. We also propose a novel reparametrization of the latent space consisting of a mixture of discrete and continuous variables. One key takeaway is that our method generalizes better across different datasets without using any pre-training or learnt models, unlike existing methods, allowing it to be trained from scratch in an end-to-end manner. We verify our efficacy and generalizability experimentally by achieving state-of-the-art results among unsupervised methods on a variety of datasets. To the best of our knowledge, we are the first to pursue image clustering using VAEs in a purely unsupervised manner on real image datasets.},
  archiveprefix = {arxiv},
  langid = {english},
  keywords = {Computer Science - Computer Vision and Pattern Recognition},
  file = {/home/jacob/snap/zotero-snap/common/Zotero/storage/8VZJMIAF/Prasad et al. - 2020 - Variational Clustering Leveraging Variational Aut.pdf}
}

@misc{rezendeStochasticBackpropagationApproximate2014,
  title = {Stochastic {{Backpropagation}} and {{Approximate Inference}} in {{Deep Generative Models}}},
  author = {Rezende, Danilo Jimenez and Mohamed, Shakir and Wierstra, Daan},
  year = {2014},
  month = may,
  number = {arXiv:1401.4082},
  eprint = {1401.4082},
  primaryclass = {cs, stat},
  publisher = {{arXiv}},
  urldate = {2023-10-04},
  abstract = {We marry ideas from deep neural networks and approximate Bayesian inference to derive a generalised class of deep, directed generative models, endowed with a new algorithm for scalable inference and learning. Our algorithm introduces a recognition model to represent an approximate posterior distribution and uses this for optimisation of a variational lower bound. We develop stochastic backpropagation \textendash{} rules for gradient backpropagation through stochastic variables \textendash{} and derive an algorithm that allows for joint optimisation of the parameters of both the generative and recognition models. We demonstrate on several real-world data sets that by using stochastic backpropagation and variational inference, we obtain models that are able to generate realistic samples of data, allow for accurate imputations of missing data, and provide a useful tool for high-dimensional data visualisation.},
  archiveprefix = {arxiv},
  langid = {english},
  keywords = {Bayesian Inference,Computer Science - Artificial Intelligence,Computer Science - Machine Learning,Statistics - Computation,Statistics - Machine Learning,Statistics - Methodology,VAEs,Variational Inference},
  file = {/home/jacob/snap/zotero-snap/common/Zotero/storage/DTV3VTLA/Rezende et al. - 2014 - Stochastic Backpropagation and Approximate Inferen.pdf}
}

@article{rhodesScientificSynergyLSST2017,
  title = {Scientific {{Synergy}} between {{LSST}} and {{{\emph{Euclid}}}}},
  author = {Rhodes, Jason and Nichol, Robert C. and Aubourg, {\'E}ric and Bean, Rachel and Boutigny, Dominique and Bremer, Malcolm N. and Capak, Peter and Cardone, Vincenzo and Carry, Beno{\^i}t and Conselice, Christopher J. and Connolly, Andrew J. and Cuillandre, Jean-Charles and Hatch, N. A. and Helou, George and Hemmati, Shoubaneh and Hildebrandt, Hendrik and Hlo{\v z}ek, Ren{\'e}e and Jones, Lynne and Kahn, Steven and Kiessling, Alina and Kitching, Thomas and Lupton, Robert and Mandelbaum, Rachel and Markovic, Katarina and Marshall, Phil and Massey, Richard and Maughan, Ben J. and Melchior, Peter and Mellier, Yannick and Newman, Jeffrey A. and Robertson, Brant and Sauvage, Marc and Schrabback, Tim and Smith, Graham P. and Strauss, Michael A. and Taylor, Andy and Linden, Anja Von Der},
  year = {2017},
  month = dec,
  journal = {The Astrophysical Journal Supplement Series},
  volume = {233},
  number = {2},
  pages = {21},
  issn = {1538-4365},
  doi = {10.3847/1538-4365/aa96b0},
  urldate = {2022-02-21},
  abstract = {Euclid and the Large Synoptic Survey Telescope (LSST) are poised to dramatically change the astronomy landscape early in the next decade. The combination of high-cadence, deep, wide-field optical photometry from LSST with highresolution, wide-field optical photometry, and near-infrared photometry and spectroscopy from Euclid will be powerful for addressing a wide range of astrophysical questions. We explore Euclid/LSST synergy, ignoring the political issues associated with data access to focus on the scientific, technical, and financial benefits of coordination. We focus primarily on dark energy cosmology, but also discuss galaxy evolution, transient objects, solar system science, and galaxy cluster studies. We concentrate on synergies that require coordination in cadence or survey overlap, or would benefit from pixellevel co-processing that is beyond the scope of what is currently planned, rather than scientific programs that could be accomplished only at the catalog level without coordination in data processing or survey strategies. We provide two quantitative examples of scientific synergies: the decrease in photo-z errors (benefiting many science cases) when highresolution Euclid data are used for LSST photo-z determination, and the resulting increase in weak-lensing signal-to-noise ratio from smaller photo-z errors. We briefly discuss other areas of coordination, including high-performance computing resources and calibration data. Finally, we address concerns about the loss of independence and potential cross-checks between the two missions and the potential consequences of not collaborating.},
  langid = {english},
  file = {/home/jacob/snap/zotero-snap/common/Zotero/storage/P7MIFLIX/Rhodes et al. - 2017 - Scientific Synergy between LSST and Euclid.pdf}
}

@misc{salvatoManyFlavoursPhotometric2018,
  title = {The Many Flavours of Photometric Redshifts},
  author = {Salvato, Mara and Ilbert, Olivier and Hoyle, Ben},
  year = {2018},
  month = jun,
  number = {arXiv:1805.12574},
  eprint = {1805.12574},
  primaryclass = {astro-ph},
  publisher = {{arXiv}},
  urldate = {2023-10-11},
  abstract = {For more that seventy years, the measurements of fluxes of galaxies at different wavelengths and derived colours have been used to estimate their corresponding cosmological distances. From the fields of galaxy and AGN evolution to precision cosmology, the number of scientific projects relying on such distance measurements, called photometric redshifts, have exploded. The benefits of photometric redshifts is that all sources detected in photometric images can have some distance estimates relatively cheaply. The major drawback is that these cheap estimates have a low precision when compared with the resource-expensive spectroscopy. The methodology to estimate redshifts has been through several major revolutions throughout the last decades, triggered by increasingly more stringent requirements on the photometric redshift accuracy. Here, we review the various techniques to obtain photometric redshifts, from template-fitting to machine learning and hybrid systems. We also describe the state-of-the-art results on current extra-galactic samples and explain how survey strategy choices impact redshift accuracy. We close the review with a description of the photometric redshifts efforts planned for upcoming wide field surveys, which will collect data on billions of galaxies, aiming to solve the most exciting cosmological and astrophysical questions of today.},
  archiveprefix = {arxiv},
  langid = {english},
  keywords = {Astrophysics - Astrophysics of Galaxies},
  file = {/home/jacob/snap/zotero-snap/common/Zotero/storage/BM83TXLW/Salvato et al. - 2018 - The many flavours of photometric redshifts.pdf}
}

@article{schmidtEvaluationProbabilisticPhotometric2020,
  title = {Evaluation of Probabilistic Photometric Redshift Estimation Approaches for {{The Rubin Observatory Legacy Survey}} of {{Space}} and {{Time}} ({{LSST}})},
  author = {Schmidt, S J and Malz, A I and Soo, J Y H and Almosallam, I A and Brescia, M and Cavuoti, S and {Cohen-Tanugi}, J and Connolly, A J and DeRose, J and Freeman, P E and Graham, M L and Iyer, K G and Jarvis, M J and Kalmbach, J B and Kovacs, E and Lee, A B and Longo, G and Morrison, C B and Newman, J A and Nourbakhsh, E and Nuss, E and Pospisil, T and Tranin, H and Wechsler, R H and Zhou, R and Izbicki, R},
  year = {2020},
  month = sep,
  journal = {Monthly Notices of the Royal Astronomical Society},
  pages = {staa2799},
  issn = {0035-8711, 1365-2966},
  doi = {10.1093/mnras/staa2799},
  urldate = {2022-10-19},
  abstract = {Many scientific investigations of photometric galaxy surveys require redshift estimates, whose uncertainty properties are best encapsulated by photometric redshift (photo-z) posterior probability density functions (PDFs). A plethora of photo-z PDF estimation methodologies abound, producing discrepant results with no consensus on a preferred approach. We present the results of a comprehensive experiment comparing 12 photo-z algorithms applied to mock data produced for The Rubin Observatory Legacy Survey of Space and Time Dark Energy Science Collaboration. By supplying perfect prior information, in the form of the complete template library and a representative training set as inputs to each code, we demonstrate the impact of the assumptions underlying each technique on the output photo-z PDFs. In the absence of a notion of true, unbiased photo-z PDFs, we evaluate and interpret multiple metrics of the ensemble properties of the derived photo-z PDFs as well as traditional reductions to photo-z point estimates. We report systematic biases and overall over/underbreadth of the photo-z PDFs of many popular codes, which may indicate avenues for improvement in the algorithms or implementations. Furthermore, we raise attention to the limitations of established metrics for assessing photo-z PDF accuracy; though we identify the conditional density estimate loss as a promising metric of photo-z PDF performance in the case where true redshifts are available but true photo-z PDFs are not, we emphasize the need for science-specific performance metrics.},
  langid = {english},
  file = {/home/jacob/snap/zotero-snap/common/Zotero/storage/G67Y89WE/Schmidt et al. - 2020 - Evaluation of probabilistic photometric redshift e.pdf}
}

@misc{SettingLearningRate2018,
  title = {Setting the Learning Rate of Your Neural Network.},
  year = {2018},
  month = mar,
  journal = {Jeremy Jordan},
  urldate = {2022-11-04},
  abstract = {In previous posts, I've discussed how we can train neural networks using backpropagation with gradient descent. One of the key hyperparameters to set in order to train a neural network is the learning rate for gradient descent. As a reminder, this parameter scales the magnitude of our weight updates in},
  howpublished = {https://www.jeremyjordan.me/nn-learning-rate/},
  langid = {english},
  file = {/home/jacob/snap/zotero-snap/common/Zotero/storage/3CPTWCIQ/nn-learning-rate.html}
}

@misc{sharmaIncorporatingUnlabelledData2023,
  title = {Incorporating {{Unlabelled Data}} into {{Bayesian Neural Networks}}},
  author = {Sharma, Mrinank and Rainforth, Tom and Teh, Yee Whye and Fortuin, Vincent},
  year = {2023},
  month = may,
  number = {arXiv:2304.01762},
  eprint = {2304.01762},
  primaryclass = {cs, stat},
  publisher = {{arXiv}},
  urldate = {2023-10-04},
  abstract = {Conventional Bayesian Neural Networks (BNNs) cannot leverage unlabelled data to improve their predictions. To overcome this limitation, we introduce SelfSupervised Bayesian Neural Networks, which use unlabelled data to learn improved prior predictive distributions by maximising an evidence lower bound during an unsupervised pre-training step. With a novel methodology developed to better understand prior predictive distributions, we then show that self-supervised prior predictives capture image semantics better than conventional BNN priors. In our empirical evaluations, we see that self-supervised BNNs offer the label efficiency of self-supervised methods and the uncertainty estimates of Bayesian methods, particularly outperforming conventional BNNs in low-to-medium data regimes.},
  archiveprefix = {arxiv},
  langid = {english},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {/home/jacob/snap/zotero-snap/common/Zotero/storage/6X5QMJG2/Sharma et al. - 2023 - Incorporating Unlabelled Data into Bayesian Neural.pdf}
}

@misc{siddharthLearningDisentangledRepresentations2017,
  title = {Learning {{Disentangled Representations}} with {{Semi-Supervised Deep Generative Models}}},
  author = {Siddharth, N. and Paige, Brooks and {van de Meent}, Jan-Willem and Desmaison, Alban and Goodman, Noah D. and Kohli, Pushmeet and Wood, Frank and Torr, Philip H. S.},
  year = {2017},
  month = nov,
  number = {arXiv:1706.00400},
  eprint = {1706.00400},
  primaryclass = {cs, stat},
  publisher = {{arXiv}},
  urldate = {2023-10-04},
  abstract = {Variational autoencoders (VAEs) learn representations of data by jointly training a probabilistic encoder and decoder network. Typically these models encode all features of the data into a single variable. Here we are interested in learning disentangled representations that encode distinct aspects of the data into separate variables. We propose to learn such representations using model architectures that generalise from standard VAEs, employing a general graphical model structure in the encoder and decoder. This allows us to train partially-specified models that make relatively strong assumptions about a subset of interpretable variables and rely on the flexibility of neural networks to learn representations for the remaining variables. We further define a general objective for semi-supervised learning in this model class, which can be approximated using an importance sampling procedure. We evaluate our framework's ability to learn disentangled representations, both by qualitative exploration of its generative capacity, and quantitative evaluation of its discriminative ability on a variety of models and datasets.},
  archiveprefix = {arxiv},
  langid = {english},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {/home/jacob/snap/zotero-snap/common/Zotero/storage/BYGKXJQB/Siddharth et al. - 2017 - Learning Disentangled Representations with Semi-Su.pdf}
}

@misc{sinhaConsistencyRegularizationVariational2022,
  title = {Consistency {{Regularization}} for {{Variational Auto-Encoders}}},
  author = {Sinha, Samarth and Dieng, Adji B.},
  year = {2022},
  month = jun,
  number = {arXiv:2105.14859},
  eprint = {2105.14859},
  primaryclass = {cs},
  publisher = {{arXiv}},
  urldate = {2023-09-29},
  abstract = {Variational auto-encoders (VAEs) are a powerful approach to unsupervised learning. They enable scalable approximate posterior inference in latent-variable models using variational inference (VI). A VAE posits a variational family parameterized by a deep neural network called an encoder that takes data as input. This encoder is shared across all the observations, which amortizes the cost of inference. However the encoder of a VAE has the undesirable property that it maps a given observation and a semantics-preserving transformation of it to different latent representations. This "inconsistency" of the encoder lowers the quality of the learned representations, especially for downstream tasks, and also negatively affects generalization. In this paper, we propose a regularization method to enforce consistency in VAEs. The idea is to minimize the Kullback-Leibler (KL) divergence between the variational distribution when conditioning on the observation and the variational distribution when conditioning on a random semantic-preserving transformation of this observation. This regularization is applicable to any VAE. In our experiments we apply it to four different VAE variants on several benchmark datasets and found it always improves the quality of the learned representations but also leads to better generalization. In particular, when applied to the Nouveau Variational Auto-Encoder (NVAE), our regularization method yields state-of-the-art performance on MNIST and CIFAR-10. We also applied our method to 3D data and found it learns representations of superior quality as measured by accuracy on a downstream classification task.},
  archiveprefix = {arxiv},
  langid = {english},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning},
  file = {/home/jacob/snap/zotero-snap/common/Zotero/storage/X2LAPJ9W/Sinha and Dieng - 2022 - Consistency Regularization for Variational Auto-En.pdf}
}

@article{sonderbyLadderVariationalAutoencoders,
  title = {Ladder {{Variational Autoencoders}}},
  author = {S{\o}nderby, Casper Kaae and Raiko, Tapani and Maal{\o}e, Lars and S{\o}nderby, S{\o}ren Kaae and Winther, Ole},
  abstract = {Variational autoencoders are powerful models for unsupervised learning. However deep models with several layers of dependent stochastic variables are difficult to train which limits the improvements obtained using these highly expressive models. We propose a new inference model, the Ladder Variational Autoencoder, that recursively corrects the generative distribution by a data dependent approximate likelihood in a process resembling the recently proposed Ladder Network. We show that this model provides state of the art predictive log-likelihood and tighter log-likelihood lower bound compared to the purely bottom-up inference in layered Variational Autoencoders and other generative models. We provide a detailed analysis of the learned hierarchical latent representation and show that our new inference model is qualitatively different and utilizes a deeper more distributed hierarchy of latent variables. Finally, we observe that batch-normalization and deterministic warm-up (gradually turning on the KL-term) are crucial for training variational models with many stochastic layers.},
  langid = {english},
  file = {/home/jacob/snap/zotero-snap/common/Zotero/storage/FGG87IPN/Sønderby et al. - Ladder Variational Autoencoders.pdf}
}

@misc{vahdatNVAEDeepHierarchical2021,
  title = {{{NVAE}}: {{A Deep Hierarchical Variational Autoencoder}}},
  shorttitle = {{{NVAE}}},
  author = {Vahdat, Arash and Kautz, Jan},
  year = {2021},
  month = jan,
  number = {arXiv:2007.03898},
  eprint = {2007.03898},
  primaryclass = {cs, stat},
  publisher = {{arXiv}},
  urldate = {2023-10-15},
  abstract = {Normalizing flows, autoregressive models, variational autoencoders (VAEs), and deep energy-based models are among competing likelihood-based frameworks for deep generative learning. Among them, VAEs have the advantage of fast and tractable sampling and easy-to-access encoding networks. However, they are currently outperformed by other models such as normalizing flows and autoregressive models. While the majority of the research in VAEs is focused on the statistical challenges, we explore the orthogonal direction of carefully designing neural architectures for hierarchical VAEs. We propose Nouveau VAE (NVAE), a deep hierarchical VAE built for image generation using depth-wise separable convolutions and batch normalization. NVAE is equipped with a residual parameterization of Normal distributions and its training is stabilized by spectral regularization. We show that NVAE achieves state-of-the-art results among non-autoregressive likelihood-based models on the MNIST, CIFAR-10, CelebA 64, and CelebA HQ datasets and it provides a strong baseline on FFHQ. For example, on CIFAR-10, NVAE pushes the state-of-the-art from 2.98 to 2.91 bits per dimension, and it produces high-quality images on CelebA HQ as shown in Fig. 1. To the best of our knowledge, NVAE is the first successful VAE applied to natural images as large as 256\texttimes 256 pixels. The source code is available at https://github.com/NVlabs/NVAE.},
  archiveprefix = {arxiv},
  langid = {english},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {/home/jacob/snap/zotero-snap/common/Zotero/storage/9VGDBQAJ/Vahdat and Kautz - 2021 - NVAE A Deep Hierarchical Variational Autoencoder.pdf}
}

@software{blackjax2020github,
  author = {Cabezas, Alberto and Lao, Junpeng and Louf, R\'emi},
  title = {{B}lackjax: A sampling library for {JAX}},
  url = {http://github.com/blackjax-devs/blackjax},
  version = {<insert current release tag>},
  year = {2023},
}